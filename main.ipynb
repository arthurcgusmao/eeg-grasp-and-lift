{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#np.random.seed(0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(sbj_i, srs_i):\n",
    "    x = pd.read_csv('./data/train/subj{}_series{}_data.csv'  .format(sbj_i, srs_i), index_col='id')\n",
    "    y = pd.read_csv('./data/train/subj{}_series{}_events.csv'.format(sbj_i, srs_i), index_col='id')\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 96 series in total.\n"
     ]
    }
   ],
   "source": [
    "data = [read_data(i,j) for j in range(1,9) for i in range(1,13)]\n",
    "print('There are {} series in total.'.format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "all_x = pd.DataFrame()\n",
    "for x,_ in data:\n",
    "    all_x = pd.concat((x, all_x))\n",
    "\n",
    "scaler.fit(all_x)\n",
    "train = []\n",
    "for x,y in data:\n",
    "    train.append([scaler.transform(x),y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualization\n",
    "# xcols = data[0][0].columns\n",
    "# fig = plt.figure(figsize=(25,200))\n",
    "\n",
    "# for idx,col in enumerate(xcols):\n",
    "#     xx = []\n",
    "#     ax = fig.add_subplot(len(xcols), 1, idx+1)\n",
    "#     for x,y in train:\n",
    "#         xx.append(x[col])\n",
    "# #         break\n",
    "#     ax.boxplot(xx, showfliers=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into Train/Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select one series from each subject to be part of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idxs = reversed([0,14,23,25,37,44,50,57,65,78,80,94])\n",
    "valid = [train.pop(i) for i in valid_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9353076 , -0.05334334, -1.72135124, -0.80666212,  0.06618727,\n",
       "         0.54386744,  0.12806491, -0.48223347,  1.32653733,  1.32760938,\n",
       "         0.39085719, -0.89306585,  4.73817947,  2.156778  ,  1.19188958,\n",
       "         1.11753778, -0.44699572, -0.74099763,  1.83291025,  0.83643326,\n",
       "         1.04905821,  1.34776829,  0.49217662,  2.77917604,  1.70834393,\n",
       "         0.4847347 ,  0.5245576 ,  0.27292517,  0.99660636,  1.8774586 ,\n",
       "         0.26778848,  0.58820446],\n",
       "       [ 0.93339755, -0.01907283, -1.66025906, -0.66851354,  0.06053278,\n",
       "         0.7010149 ,  0.12395646, -0.39446198,  1.56226182,  1.45132501,\n",
       "         0.48487921, -0.93837847,  4.85866213,  2.27391795,  1.2843401 ,\n",
       "         1.14263616, -0.34685444, -0.65210452,  1.93424751,  0.91453691,\n",
       "         1.09427202,  1.24742336,  0.62571872,  2.85492979,  1.68338706,\n",
       "         0.52078477,  0.42579472,  0.56862171,  1.00883616,  1.84420457,\n",
       "         0.36146941,  0.6180799 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HandStart</th>\n",
       "      <th>FirstDigitTouch</th>\n",
       "      <th>BothStartLoadPhase</th>\n",
       "      <th>LiftOff</th>\n",
       "      <th>Replace</th>\n",
       "      <th>BothReleased</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subj2_series1_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subj2_series1_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 HandStart  FirstDigitTouch  BothStartLoadPhase  LiftOff  \\\n",
       "id                                                                         \n",
       "subj2_series1_0          0                0                   0        0   \n",
       "subj2_series1_1          0                0                   0        0   \n",
       "\n",
       "                 Replace  BothReleased  \n",
       "id                                      \n",
       "subj2_series1_0        0             0  \n",
       "subj2_series1_1        0             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = train[0]\n",
    "display(x[:2])\n",
    "display(y.iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Data Exploration\n",
    "\n",
    "In this section we are going to explore the data, checking for possible label imbalances for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average series length: 187071\n",
      "Sum of all series length: 15714003\n"
     ]
    }
   ],
   "source": [
    "len_ = 0\n",
    "count = 0\n",
    "for x,y in train:\n",
    "    len_ += len(x)\n",
    "    count += 1\n",
    "print(\"Average series length: {}\".format(len_/count))\n",
    "print(\"Sum of all series length: {}\".format(len_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAD8CAYAAAAyun5JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV0REJExUdEVERMLk/wfgMB2VfdtQbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0e77d0750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "cols = train[0][1].columns\n",
    "first_ax = None\n",
    "\n",
    "for idx,col in enumerate(cols):\n",
    "    if first_ax == None:\n",
    "        ax = fig.add_subplot(len(cols),1,idx+1)\n",
    "        first_ax = ax\n",
    "    else: \n",
    "        ax = fig.add_subplot(len(cols),1,idx+1, sharex=first_ax)\n",
    "    ax.plot(train[33][1].reset_index()[col][:4000], label=col)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above we have plotted how each label evolve over time for one repetition of the grasp-and-lift movement. We can see that there is some overlapping of labels, specially for the \"FirstDigitTouch\", \"BothStartLoadPhase\" and \"LiftOff\" labels. Therefore, we must NOT apply a softmax function at our outputs, but a sigmoid instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HandStart': 150, 'LiftOff': 150, 'Replace': 150, 'BothStartLoadPhase': 150, 'FirstDigitTouch': 150, 'BothReleased': 150}\n"
     ]
    }
   ],
   "source": [
    "last = {}\n",
    "step_start = {}\n",
    "step_duration = {}\n",
    "step_count = {}\n",
    "y_cols = train[0][1].columns\n",
    "for col in y_cols:\n",
    "    step_start[col] = 0\n",
    "    step_duration[col] = 0\n",
    "    step_count[col] = 0\n",
    "    \n",
    "for _,y in [train[0]]:\n",
    "    for idx,row in y.reset_index().iterrows():\n",
    "        for col in y_cols:\n",
    "            if idx == 0:\n",
    "                pass\n",
    "            elif last[col] == 0 and row[col] == 1:\n",
    "                step_start[col] = idx\n",
    "            elif last[col] == 1 and row[col] == 0:\n",
    "                step_duration[col] += idx - step_start[col]\n",
    "                step_count[col] += 1\n",
    "            last[col] = row[col]\n",
    "\n",
    "step_avg_duration = {}\n",
    "for col in y_cols:\n",
    "    step_avg_duration[col] = step_duration[col] / step_count[col]\n",
    "\n",
    "print step_avg_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duration of each step (the total amount of time each movement is active) is 150 for all series. We use this information to guide our search for the best window size (WS). A WS smaller than 150 is likely to miss important information that would help to identify an ongoing movement. WSs much bigger than that are likely to fill the model with unnecessary information (increasing variance). Thus, we expect to experiment with WSs ranging from 150 to 1500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Class (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, model_def, window_size=None):\n",
    "        # define tensorflow graph\n",
    "        if window_size != None:\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, window_size, 32])\n",
    "            self.logits = model_def(self.inputs, window_size)\n",
    "        else:\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, 32])\n",
    "            self.logits = model_def(self.inputs)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.float32, [None,  6])\n",
    "        self.preds = tf.nn.sigmoid(self.logits)      \n",
    "        self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.labels))\n",
    "        \n",
    "    \n",
    "    def set_data(self, train, valid):\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        \n",
    "    def fit(self, epochs, batch_size, batches_gen, lr=0.001):\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.cost)\n",
    "        self.sess = tf.Session()\n",
    "        start_time = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # loop through epochs\n",
    "        for e in range(epochs):\n",
    "            loss = 0\n",
    "            n_total_batches = 0\n",
    "            partial_loss = 0; partial_n_batches_count = 0\n",
    "            print_partial_loss_every = 1000000//batch_size\n",
    "#             print_validation_every = 5500000//batch_size # 5.5M is around 1/3 of an epoch\n",
    "            # loop through training series (and subjects)\n",
    "            for xx,yy in batches_gen(self.train, batch_size, self.window_size, shuffle=True):\n",
    "                _, res = self.sess.run([self.optimizer, self.cost], feed_dict={self.inputs: xx, self.labels: yy}) # train (update weights)\n",
    "                loss += res # calculate loss\n",
    "                n_total_batches += 1\n",
    "                partial_loss += res; partial_n_batches_count += 1 # calculate partial loss\n",
    "                \n",
    "                if n_total_batches % print_partial_loss_every == 0:\n",
    "                    loss_print = partial_loss/partial_n_batches_count # get mean loss to make loss comparable among different batch sizes\n",
    "                    print(\"Epoch: {},\\tBatch: {},\\tMean Loss (last batches): {}\".format(e, n_total_batches, loss_print))\n",
    "                    partial_loss = 0; partial_n_batches_count = 0\n",
    "#                 if n_total_batches % print_validation_every == 0:\n",
    "#                     print(\"Validating...\")\n",
    "#                     valid_mean_auc = self.validate(batch_size, batches_gen)\n",
    "#                     print(\"Valid Mean AUC: {}\".format(valid_mean_auc))\n",
    "            \n",
    "            # print train and valid information\n",
    "            loss /= n_total_batches # get mean loss to make loss comparable among different batch sizes\n",
    "            valid_mean_auc = self.validate(batch_size, batches_gen)\n",
    "            print(\"-----\")\n",
    "            print(\"Epoch: {},\\tBatch: ----,\\tMean Loss (epoch): {},\\tValid Mean AUC: {}\".format(e, loss, valid_mean_auc))\n",
    "            print(\"-----\")\n",
    "\n",
    "        train_time = time.time() - start_time\n",
    "        print(\"Model was trained in {} seconds.\".format(train_time))\n",
    "        \n",
    "        \n",
    "    def validate(self, batch_size, batches_gen):\n",
    "        \"\"\"Calculates the mean ROC AUC.\"\"\"\n",
    "        total_predictions = []\n",
    "        total_labels = pd.DataFrame()\n",
    "        for xx,yy in batches_gen(self.valid, batch_size, self.window_size, shuffle=False):\n",
    "            predictions = self.sess.run(self.preds, feed_dict={self.inputs: xx})\n",
    "            total_predictions.extend(predictions)\n",
    "            total_labels = pd.concat((total_labels, yy))\n",
    "        mean_auc = roc_auc_score(total_labels, total_predictions, average='macro')\n",
    "        return mean_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_nn(inputs):\n",
    "    l1 = tf.layers.dense(inputs, 16)\n",
    "    l2 = tf.layers.dense(l1    , 10)\n",
    "    l3 = tf.layers.dense(l2    ,  6)\n",
    "    return l3\n",
    "            \n",
    "def feedforward_batches_gen(xy_series, batch_size, window_size=None, shuffle=False):\n",
    "    \"\"\"Generate batches of length `batch_size` from a list of (features, labels) tuples.\"\"\"\n",
    "    xy_indexes = np.arange(len(xy_series))\n",
    "    if shuffle: np.random.shuffle(xy_indexes)\n",
    "    for i in xy_indexes:\n",
    "        x,y = xy_series[i]\n",
    "        n_batches = (len(x) + batch_size - 1) // batch_size\n",
    "        for b in range(n_batches): # ceil division to do not discard datapoints\n",
    "            # slice features and labels to the current batch interval\n",
    "            xx = x[batch_size*b : batch_size*(b+1)]\n",
    "            yy = y[batch_size*b : batch_size*(b+1)]\n",
    "            yield xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_nn(inputs, window_size, n_filters=64, kernel_size=3, strides=1, dense_size=128): # Bx1024x32 (supposing window_size=1024 and features=32)\n",
    "    lconv = tf.layers.conv1d(inputs, filters=n_filters, kernel_size=kernel_size, strides=strides, activation=tf.nn.relu, padding='same') # Bx1024x64\n",
    "    lpool = tf.layers.max_pooling1d(lconv, pool_size=2, strides=2, padding='same') # Bx512x64\n",
    "    lflat = tf.reshape(lpool, [-1, (window_size/2)*n_filters/strides]) # Bx(512*64)\n",
    "    ldense = tf.layers.dense(lflat, dense_size, activation=tf.nn.relu) # Bx128\n",
    "    logits = tf.layers.dense(ldense, 6) # Bx6\n",
    "    return logits\n",
    "\n",
    "def conv_nn_2(inputs, window_size): # Bx1024x32 (supposing window_size=1024 and features=32)\n",
    "    # b x ws x 32\n",
    "    lconv = tf.layers.conv1d(inputs, filters=32, kernel_size=3, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    # b x ws/2 x 32\n",
    "    lconv2 = tf.layers.conv1d(lconv, filters=16, kernel_size=3, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    # b x ws/4 x 16\n",
    "    lpool = tf.layers.max_pooling1d(lconv2, pool_size=2, strides=2, padding='same') # pooling does not affect channels\n",
    "    # b x ws/8 x 16\n",
    "    lflat = tf.reshape(lpool, [-1, (window_size/8)*16]) # flatten to apply dense layer\n",
    "    ldense1 = tf.layers.dense(lflat, 256, activation=tf.nn.relu)\n",
    "    ldense2 = tf.layers.dense(ldense1, 32, activation=tf.nn.relu) \n",
    "    logits = tf.layers.dense(ldense2, 6)\n",
    "    return logits\n",
    "\n",
    "def strided_axis0(a, L):\n",
    "    \"\"\"Creates a view into the input array.\n",
    "    Taken from https://stackoverflow.com/a/43413801/5103881\n",
    "    \"\"\"\n",
    "    # Store the shape and strides info\n",
    "    shp = a.shape\n",
    "    s  = a.strides\n",
    "\n",
    "    # Compute length of output array along the first axis\n",
    "    nd0 = shp[0]-L+1\n",
    "\n",
    "    # Setup shape and strides for use with np.lib.stride_tricks.as_strided\n",
    "    # and get (n+1) dim output array\n",
    "    shp_in = (nd0,L)+shp[1:]\n",
    "    strd_in = (s[0],) + s\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shp_in, strides=strd_in, writeable=False)\n",
    "\n",
    "def conv_batches_gen(xy_series, batch_size, window_size, shuffle=False):\n",
    "    \"\"\"Generate batches of length `batch_size` from a list of (features, labels) tuples.\"\"\"\n",
    "    xy_indexes = np.arange(len(xy_series))\n",
    "    if shuffle: np.random.shuffle(xy_indexes)\n",
    "    for i in xy_indexes:\n",
    "        x,y = xy_series[i]\n",
    "        x_slices = strided_axis0(x, window_size)\n",
    "        y_slices = y.iloc[window_size-1:] # we cannot fit any data point before (window_size - 1)\n",
    "        n_batches = (len(x_slices) + batch_size - 1) // batch_size # trick to ceil using floor division\n",
    "        for b in range(n_batches):\n",
    "            yield x_slices[batch_size*b : batch_size*(b+1)], y_slices.iloc[batch_size*b : batch_size*(b+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 subjects and 8 series. To train using all data takes a long period of time. In order to speed up the model selection and hyperparameter tuning we use data from one subject only (subject #1). This corresponds to taking the first 7 training series and the first validation series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ms = train\n",
    "valid_ms = valid[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,\tBatch: 976,\tMean Loss (last batches): 0.145131434088\n",
      "Epoch: 0,\tBatch: 1952,\tMean Loss (last batches): 0.112968327311\n",
      "Epoch: 0,\tBatch: 2928,\tMean Loss (last batches): 0.12677082369\n",
      "Epoch: 0,\tBatch: 3904,\tMean Loss (last batches): 0.0796978171415\n",
      "Epoch: 0,\tBatch: 4880,\tMean Loss (last batches): 0.104712334079\n",
      "Epoch: 0,\tBatch: 5856,\tMean Loss (last batches): 0.0877261946894\n",
      "Epoch: 0,\tBatch: 6832,\tMean Loss (last batches): 0.113694066458\n",
      "Epoch: 0,\tBatch: 7808,\tMean Loss (last batches): 0.103212157959\n",
      "Epoch: 0,\tBatch: 8784,\tMean Loss (last batches): 0.097332759139\n",
      "Epoch: 0,\tBatch: 9760,\tMean Loss (last batches): 0.0687983191268\n",
      "Epoch: 0,\tBatch: 10736,\tMean Loss (last batches): 0.100042946674\n",
      "Epoch: 0,\tBatch: 11712,\tMean Loss (last batches): 0.0908773105772\n",
      "Epoch: 0,\tBatch: 12688,\tMean Loss (last batches): 0.0894502694037\n",
      "Epoch: 0,\tBatch: 13664,\tMean Loss (last batches): 0.0670513886708\n",
      "Epoch: 0,\tBatch: 14640,\tMean Loss (last batches): 0.104065466332\n",
      "-----\n",
      "Epoch: 0,\tBatch: ----,\tMean Loss (epoch): 0.0992636054824,\tValid Mean AUC: 0.741668310011\n",
      "-----\n",
      "Epoch: 1,\tBatch: 976,\tMean Loss (last batches): 0.12016375988\n",
      "Epoch: 1,\tBatch: 1952,\tMean Loss (last batches): 0.0956159984615\n",
      "Epoch: 1,\tBatch: 2928,\tMean Loss (last batches): 0.0844165264999\n",
      "Epoch: 1,\tBatch: 3904,\tMean Loss (last batches): 0.079466467808\n",
      "Epoch: 1,\tBatch: 4880,\tMean Loss (last batches): 0.112747000926\n",
      "Epoch: 1,\tBatch: 5856,\tMean Loss (last batches): 0.101928503886\n",
      "Epoch: 1,\tBatch: 6832,\tMean Loss (last batches): 0.0726699430023\n",
      "Epoch: 1,\tBatch: 7808,\tMean Loss (last batches): 0.0810439068836\n",
      "Epoch: 1,\tBatch: 8784,\tMean Loss (last batches): 0.0932032067681\n",
      "Epoch: 1,\tBatch: 9760,\tMean Loss (last batches): 0.0788461861968\n",
      "Epoch: 1,\tBatch: 10736,\tMean Loss (last batches): 0.0727937561147\n",
      "Epoch: 1,\tBatch: 11712,\tMean Loss (last batches): 0.0961043912696\n",
      "Epoch: 1,\tBatch: 12688,\tMean Loss (last batches): 0.0835297836441\n",
      "Epoch: 1,\tBatch: 13664,\tMean Loss (last batches): 0.0906141541809\n",
      "Epoch: 1,\tBatch: 14640,\tMean Loss (last batches): 0.104647241075\n",
      "-----\n",
      "Epoch: 1,\tBatch: ----,\tMean Loss (epoch): 0.0913819530708,\tValid Mean AUC: 0.844543883244\n",
      "-----\n",
      "Epoch: 2,\tBatch: 976,\tMean Loss (last batches): 0.083160112872\n",
      "Epoch: 2,\tBatch: 1952,\tMean Loss (last batches): 0.085232453713\n",
      "Epoch: 2,\tBatch: 2928,\tMean Loss (last batches): 0.0805923361369\n",
      "Epoch: 2,\tBatch: 3904,\tMean Loss (last batches): 0.0952489954268\n",
      "Epoch: 2,\tBatch: 4880,\tMean Loss (last batches): 0.0907798070747\n",
      "Epoch: 2,\tBatch: 5856,\tMean Loss (last batches): 0.0785382450364\n",
      "Epoch: 2,\tBatch: 6832,\tMean Loss (last batches): 0.0804202528625\n",
      "Epoch: 2,\tBatch: 7808,\tMean Loss (last batches): 0.0840336256751\n",
      "Epoch: 2,\tBatch: 8784,\tMean Loss (last batches): 0.101986545354\n",
      "Epoch: 2,\tBatch: 9760,\tMean Loss (last batches): 0.0957214493809\n",
      "Epoch: 2,\tBatch: 10736,\tMean Loss (last batches): 0.0781592161075\n",
      "Epoch: 2,\tBatch: 11712,\tMean Loss (last batches): 0.0940684096854\n",
      "Epoch: 2,\tBatch: 12688,\tMean Loss (last batches): 0.0852491341161\n",
      "Epoch: 2,\tBatch: 13664,\tMean Loss (last batches): 0.0836245354698\n",
      "Epoch: 2,\tBatch: 14640,\tMean Loss (last batches): 0.0825881159224\n",
      "-----\n",
      "Epoch: 2,\tBatch: ----,\tMean Loss (epoch): 0.0877312493838,\tValid Mean AUC: 0.801125609134\n",
      "-----\n",
      "Epoch: 3,\tBatch: 976,\tMean Loss (last batches): 0.0888722161921\n",
      "Epoch: 3,\tBatch: 1952,\tMean Loss (last batches): 0.0771028790101\n",
      "Epoch: 3,\tBatch: 2928,\tMean Loss (last batches): 0.0901185552642\n",
      "Epoch: 3,\tBatch: 3904,\tMean Loss (last batches): 0.101232201719\n",
      "Epoch: 3,\tBatch: 4880,\tMean Loss (last batches): 0.0751526508079\n",
      "Epoch: 3,\tBatch: 5856,\tMean Loss (last batches): 0.111002944471\n",
      "Epoch: 3,\tBatch: 6832,\tMean Loss (last batches): 0.0716080267377\n",
      "Epoch: 3,\tBatch: 7808,\tMean Loss (last batches): 0.110294632811\n",
      "Epoch: 3,\tBatch: 8784,\tMean Loss (last batches): 0.0810943815884\n",
      "Epoch: 3,\tBatch: 9760,\tMean Loss (last batches): 0.0675932632113\n",
      "Epoch: 3,\tBatch: 10736,\tMean Loss (last batches): 0.0961268594283\n",
      "Epoch: 3,\tBatch: 11712,\tMean Loss (last batches): 0.0931475286877\n",
      "Epoch: 3,\tBatch: 12688,\tMean Loss (last batches): 0.0790555777151\n",
      "Epoch: 3,\tBatch: 13664,\tMean Loss (last batches): 0.0882372749002\n",
      "Epoch: 3,\tBatch: 14640,\tMean Loss (last batches): 0.0848248129857\n",
      "-----\n",
      "Epoch: 3,\tBatch: ----,\tMean Loss (epoch): 0.0868122659212,\tValid Mean AUC: 0.632412581967\n",
      "-----\n",
      "Epoch: 4,\tBatch: 976,\tMean Loss (last batches): 0.0751815057729\n",
      "Epoch: 4,\tBatch: 1952,\tMean Loss (last batches): 0.0830959309405\n",
      "Epoch: 4,\tBatch: 2928,\tMean Loss (last batches): 0.0758872873213\n",
      "Epoch: 4,\tBatch: 3904,\tMean Loss (last batches): 0.110496311746\n",
      "Epoch: 4,\tBatch: 4880,\tMean Loss (last batches): 0.0945270222358\n",
      "Epoch: 4,\tBatch: 5856,\tMean Loss (last batches): 0.0994011756889\n",
      "Epoch: 4,\tBatch: 6832,\tMean Loss (last batches): 0.0880316048776\n",
      "Epoch: 4,\tBatch: 7808,\tMean Loss (last batches): 0.102906122562\n",
      "Epoch: 4,\tBatch: 8784,\tMean Loss (last batches): 0.0884743979858\n",
      "Epoch: 4,\tBatch: 9760,\tMean Loss (last batches): 0.100991326428\n",
      "Epoch: 4,\tBatch: 10736,\tMean Loss (last batches): 0.0698567047926\n",
      "Epoch: 4,\tBatch: 11712,\tMean Loss (last batches): 0.0624461403887\n",
      "Epoch: 4,\tBatch: 12688,\tMean Loss (last batches): 0.0771408745139\n",
      "Epoch: 4,\tBatch: 13664,\tMean Loss (last batches): 0.0930877678296\n",
      "Epoch: 4,\tBatch: 14640,\tMean Loss (last batches): 0.095327361655\n",
      "-----\n",
      "Epoch: 4,\tBatch: ----,\tMean Loss (epoch): 0.0873330849445,\tValid Mean AUC: 0.844321665688\n",
      "-----\n",
      "Epoch: 5,\tBatch: 976,\tMean Loss (last batches): 0.093663004838\n",
      "Epoch: 5,\tBatch: 1952,\tMean Loss (last batches): 0.0930637159929\n",
      "Epoch: 5,\tBatch: 2928,\tMean Loss (last batches): 0.0727587150092\n",
      "Epoch: 5,\tBatch: 3904,\tMean Loss (last batches): 0.0719090245392\n",
      "Epoch: 5,\tBatch: 4880,\tMean Loss (last batches): 0.0902922487963\n",
      "Epoch: 5,\tBatch: 5856,\tMean Loss (last batches): 0.0853074447338\n",
      "Epoch: 5,\tBatch: 6832,\tMean Loss (last batches): 0.082437892056\n",
      "Epoch: 5,\tBatch: 7808,\tMean Loss (last batches): 0.0877741625588\n",
      "Epoch: 5,\tBatch: 8784,\tMean Loss (last batches): 0.0891758957161\n",
      "Epoch: 5,\tBatch: 9760,\tMean Loss (last batches): 0.0984331461884\n",
      "Epoch: 5,\tBatch: 10736,\tMean Loss (last batches): 0.0940437942173\n",
      "Epoch: 5,\tBatch: 11712,\tMean Loss (last batches): 0.0868174930507\n",
      "Epoch: 5,\tBatch: 12688,\tMean Loss (last batches): 0.0703699585348\n",
      "Epoch: 5,\tBatch: 13664,\tMean Loss (last batches): 0.0991273654298\n",
      "Epoch: 5,\tBatch: 14640,\tMean Loss (last batches): 0.0798486220797\n",
      "-----\n",
      "Epoch: 5,\tBatch: ----,\tMean Loss (epoch): 0.0860177175098,\tValid Mean AUC: 0.776361256093\n",
      "-----\n",
      "Epoch: 6,\tBatch: 976,\tMean Loss (last batches): 0.0844430495517\n",
      "Epoch: 6,\tBatch: 1952,\tMean Loss (last batches): 0.0786969577781\n",
      "Epoch: 6,\tBatch: 2928,\tMean Loss (last batches): 0.0687428052977\n",
      "Epoch: 6,\tBatch: 3904,\tMean Loss (last batches): 0.0894378651197\n",
      "Epoch: 6,\tBatch: 4880,\tMean Loss (last batches): 0.0655007168854\n",
      "Epoch: 6,\tBatch: 5856,\tMean Loss (last batches): 0.0671864348005\n",
      "Epoch: 6,\tBatch: 6832,\tMean Loss (last batches): 0.0949553918692\n",
      "Epoch: 6,\tBatch: 7808,\tMean Loss (last batches): 0.0919505422244\n",
      "Epoch: 6,\tBatch: 8784,\tMean Loss (last batches): 0.0865235780766\n",
      "Epoch: 6,\tBatch: 9760,\tMean Loss (last batches): 0.102304238329\n",
      "Epoch: 6,\tBatch: 10736,\tMean Loss (last batches): 0.0796887062705\n",
      "Epoch: 6,\tBatch: 11712,\tMean Loss (last batches): 0.0995063796027\n",
      "Epoch: 6,\tBatch: 12688,\tMean Loss (last batches): 0.0841089051953\n",
      "Epoch: 6,\tBatch: 13664,\tMean Loss (last batches): 0.089204876187\n",
      "Epoch: 6,\tBatch: 14640,\tMean Loss (last batches): 0.0867595910347\n",
      "-----\n",
      "Epoch: 6,\tBatch: ----,\tMean Loss (epoch): 0.084551565177,\tValid Mean AUC: 0.805725747037\n",
      "-----\n",
      "Epoch: 7,\tBatch: 976,\tMean Loss (last batches): 0.0803032076103\n",
      "Epoch: 7,\tBatch: 1952,\tMean Loss (last batches): 0.0825203076687\n",
      "Epoch: 7,\tBatch: 2928,\tMean Loss (last batches): 0.0674798394274\n",
      "Epoch: 7,\tBatch: 3904,\tMean Loss (last batches): 0.0815054853888\n",
      "Epoch: 7,\tBatch: 4880,\tMean Loss (last batches): 0.104102693697\n",
      "Epoch: 7,\tBatch: 5856,\tMean Loss (last batches): 0.0936471872081\n",
      "Epoch: 7,\tBatch: 6832,\tMean Loss (last batches): 0.0909652366766\n",
      "Epoch: 7,\tBatch: 7808,\tMean Loss (last batches): 0.0679927352142\n",
      "Epoch: 7,\tBatch: 8784,\tMean Loss (last batches): 0.0862899286315\n",
      "Epoch: 7,\tBatch: 9760,\tMean Loss (last batches): 0.107177637154\n",
      "Epoch: 7,\tBatch: 10736,\tMean Loss (last batches): 0.0678402204288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7,\tBatch: 11712,\tMean Loss (last batches): 0.0745107319818\n",
      "Epoch: 7,\tBatch: 12688,\tMean Loss (last batches): 0.0926317198729\n",
      "Epoch: 7,\tBatch: 13664,\tMean Loss (last batches): 0.091617249831\n",
      "Epoch: 7,\tBatch: 14640,\tMean Loss (last batches): 0.0677248359166\n",
      "-----\n",
      "Epoch: 7,\tBatch: ----,\tMean Loss (epoch): 0.0830947192922,\tValid Mean AUC: 0.788399838817\n",
      "-----\n",
      "Epoch: 8,\tBatch: 976,\tMean Loss (last batches): 0.0859610742091\n",
      "Epoch: 8,\tBatch: 1952,\tMean Loss (last batches): 0.0716566438282\n",
      "Epoch: 8,\tBatch: 2928,\tMean Loss (last batches): 0.0815766102336\n",
      "Epoch: 8,\tBatch: 3904,\tMean Loss (last batches): 0.0715843517102\n",
      "Epoch: 8,\tBatch: 4880,\tMean Loss (last batches): 0.1004473948\n",
      "Epoch: 8,\tBatch: 5856,\tMean Loss (last batches): 0.0911667081598\n",
      "Epoch: 8,\tBatch: 6832,\tMean Loss (last batches): 0.0792035839272\n",
      "Epoch: 8,\tBatch: 7808,\tMean Loss (last batches): 0.106146194623\n",
      "Epoch: 8,\tBatch: 8784,\tMean Loss (last batches): 0.0661416886828\n",
      "Epoch: 8,\tBatch: 9760,\tMean Loss (last batches): 0.0652988707289\n",
      "Epoch: 8,\tBatch: 10736,\tMean Loss (last batches): 0.0680589321204\n",
      "Epoch: 8,\tBatch: 11712,\tMean Loss (last batches): 0.094552654979\n",
      "Epoch: 8,\tBatch: 12688,\tMean Loss (last batches): 0.0749718352858\n",
      "Epoch: 8,\tBatch: 13664,\tMean Loss (last batches): 0.0985800419849\n",
      "Epoch: 8,\tBatch: 14640,\tMean Loss (last batches): 0.0911764155624\n",
      "-----\n",
      "Epoch: 8,\tBatch: ----,\tMean Loss (epoch): 0.0830012583475,\tValid Mean AUC: 0.875752623436\n",
      "-----\n",
      "Epoch: 9,\tBatch: 976,\tMean Loss (last batches): 0.0810454983911\n",
      "Epoch: 9,\tBatch: 1952,\tMean Loss (last batches): 0.0855533066057\n",
      "Epoch: 9,\tBatch: 2928,\tMean Loss (last batches): 0.101902566614\n",
      "Epoch: 9,\tBatch: 3904,\tMean Loss (last batches): 0.0894877861327\n",
      "Epoch: 9,\tBatch: 4880,\tMean Loss (last batches): 0.0699607969009\n",
      "Epoch: 9,\tBatch: 5856,\tMean Loss (last batches): 0.0764036899653\n",
      "Epoch: 9,\tBatch: 6832,\tMean Loss (last batches): 0.0838132119831\n",
      "Epoch: 9,\tBatch: 7808,\tMean Loss (last batches): 0.0813999579384\n",
      "Epoch: 9,\tBatch: 8784,\tMean Loss (last batches): 0.074520420471\n",
      "Epoch: 9,\tBatch: 9760,\tMean Loss (last batches): 0.0748806598027\n",
      "Epoch: 9,\tBatch: 10736,\tMean Loss (last batches): 0.0784093624772\n",
      "Epoch: 9,\tBatch: 11712,\tMean Loss (last batches): 0.08859094353\n",
      "Epoch: 9,\tBatch: 12688,\tMean Loss (last batches): 0.0929168135427\n",
      "Epoch: 9,\tBatch: 13664,\tMean Loss (last batches): 0.0997960891383\n",
      "Epoch: 9,\tBatch: 14640,\tMean Loss (last batches): 0.0780805722955\n",
      "-----\n",
      "Epoch: 9,\tBatch: ----,\tMean Loss (epoch): 0.0831586649004,\tValid Mean AUC: 0.857450575102\n",
      "-----\n",
      "Epoch: 10,\tBatch: 976,\tMean Loss (last batches): 0.0731747027302\n",
      "Epoch: 10,\tBatch: 1952,\tMean Loss (last batches): 0.0961479702055\n",
      "Epoch: 10,\tBatch: 2928,\tMean Loss (last batches): 0.0943298383523\n",
      "Epoch: 10,\tBatch: 3904,\tMean Loss (last batches): 0.0668691918693\n",
      "Epoch: 10,\tBatch: 4880,\tMean Loss (last batches): 0.0898048717534\n",
      "Epoch: 10,\tBatch: 5856,\tMean Loss (last batches): 0.0795141622068\n",
      "Epoch: 10,\tBatch: 6832,\tMean Loss (last batches): 0.0791704378332\n",
      "Epoch: 10,\tBatch: 7808,\tMean Loss (last batches): 0.0855358957579\n",
      "Epoch: 10,\tBatch: 8784,\tMean Loss (last batches): 0.0891542242384\n",
      "Epoch: 10,\tBatch: 9760,\tMean Loss (last batches): 0.0769037589971\n",
      "Epoch: 10,\tBatch: 10736,\tMean Loss (last batches): 0.0951207953346\n",
      "Epoch: 10,\tBatch: 11712,\tMean Loss (last batches): 0.0771912475965\n",
      "Epoch: 10,\tBatch: 12688,\tMean Loss (last batches): 0.0821933725146\n",
      "Epoch: 10,\tBatch: 13664,\tMean Loss (last batches): 0.0828527456262\n",
      "Epoch: 10,\tBatch: 14640,\tMean Loss (last batches): 0.095106722444\n",
      "-----\n",
      "Epoch: 10,\tBatch: ----,\tMean Loss (epoch): 0.0835303579368,\tValid Mean AUC: 0.750751488903\n",
      "-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6ad8459d0928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatches_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_batches_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-13-c8d8680e6e90>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, batch_size, batches_gen, lr)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# loop through training series (and subjects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train (update weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mn_total_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ltd/anaconda3/envs/openke/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ltd/anaconda3/envs/openke/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ltd/anaconda3/envs/openke/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ltd/anaconda3/envs/openke/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ltd/anaconda3/envs/openke/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ltd/anaconda3/envs/openke/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = Model(conv_nn_2, window_size=1024)\n",
    "nn.set_data(train_ms, valid_ms)\n",
    "nn.fit(\n",
    "    epochs=1000,\n",
    "    batch_size=1024,\n",
    "    batches_gen=conv_batches_gen,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.889941700921\n",
      "Elapsed time: 552.739305973\n"
     ]
    }
   ],
   "source": [
    "nn.set_data(train, valid)\n",
    "\n",
    "start_time = time.time()\n",
    "auc = nn.validate(1024, conv_batches_gen)\n",
    "print(\"AUC: {}\".format(auc))\n",
    "print('Elapsed time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07339391998329657\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "n_total_batches = 0\n",
    "for xx,yy in conv_batches_gen([train[0]], 256, 1024, shuffle=True):\n",
    "    loss += nn.sess.run(nn.cost, feed_dict={nn.inputs: xx, nn.labels: yy})\n",
    "    n_total_batches += 1\n",
    "print loss / n_total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.613213468633827"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.set_data(train, [valid[1]])\n",
    "nn.validate(256, conv_batches_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = conv_batches_gen([valid[8]], batch_size=4096, window_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.29289245605e-05\n"
     ]
    }
   ],
   "source": [
    "# testing time\n",
    "start_time = time.time()\n",
    "for x,y in g:\n",
    "    pass\n",
    "print('Elapsed time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 1024, 32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 6)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291874, 32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HandStart</th>\n",
       "      <th>FirstDigitTouch</th>\n",
       "      <th>BothStartLoadPhase</th>\n",
       "      <th>LiftOff</th>\n",
       "      <th>Replace</th>\n",
       "      <th>BothReleased</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subj2_series1_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subj2_series1_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subj2_series1_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 HandStart  FirstDigitTouch  BothStartLoadPhase  LiftOff  \\\n",
       "id                                                                         \n",
       "subj2_series1_3          0                0                   0        0   \n",
       "subj2_series1_4          0                0                   0        0   \n",
       "subj2_series1_5          0                0                   0        0   \n",
       "\n",
       "                 Replace  BothReleased  \n",
       "id                                      \n",
       "subj2_series1_3        0             0  \n",
       "subj2_series1_4        0             0  \n",
       "subj2_series1_5        0             0  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(a):\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y)[np.array([5,6,9])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = np.array([1,2,3,4,5])\n",
    "x[np.array([[1,2,3],[2,3,4]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "xx = train[0][0][0:100]\n",
    "yy = train[0][1][0:100]\n",
    "predictions = sess.run(logits, feed_dict={inputs: xx, labels: yy})\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0][1][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for x,y in valid:\n",
    "    sum_ += len(x)\n",
    "sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score([1,1,0], [0.95, 0.86, 0.02], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
